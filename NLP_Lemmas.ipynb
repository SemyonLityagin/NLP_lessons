{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qR6iTHb2hH9",
        "outputId": "6c35cf97-fa5a-404e-dbd5-ee0a7c2627ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-07 14:21:08--  https://opencorpora.org/files/export/dict/dict.opcorpora.xml.zip\n",
            "Resolving opencorpora.org (opencorpora.org)... 104.21.15.199, 172.67.163.210, 2606:4700:3030::6815:fc7, ...\n",
            "Connecting to opencorpora.org (opencorpora.org)|104.21.15.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28757314 (27M) [application/zip]\n",
            "Saving to: ‘dict.opcorpora.xml.zip’\n",
            "\n",
            "dict.opcorpora.xml. 100%[===================>]  27.42M  5.64MB/s    in 7.7s    \n",
            "\n",
            "2025-10-07 14:21:17 (3.58 MB/s) - ‘dict.opcorpora.xml.zip’ saved [28757314/28757314]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://opencorpora.org/files/export/dict/dict.opcorpora.xml.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip dict.opcorpora.xml.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnrMaULr3v4q",
        "outputId": "6bf69252-21ca-4aa5-d06a-8718080b9844"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dict.opcorpora.xml.zip\n",
            "  inflating: dict.opcorpora.xml      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import re"
      ],
      "metadata": {
        "id": "wJHkOwIu6PYg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import pickle\n",
        "import math\n",
        "from collections import Counter\n",
        "from typing import List, Sequence, Callable, Optional, Iterable, Tuple"
      ],
      "metadata": {
        "id": "1Jqk_eu-e1L4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6IqoLQyfHif",
        "outputId": "0e023684-a8fa-4f4d-e6e9-7c4e4113d026"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3-2.0.5-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.2.0)\n",
            "Downloading pymorphy3-2.0.5-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.5 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PAD = \"<PAD>\"\n",
        "\n",
        "class POSNgramFreqBuilder:\n",
        "    SENT_SPLIT_RE = re.compile(r'(?<=[.!?])\\s+')\n",
        "    WORD_RE = re.compile(r\"\\w+\", flags=re.UNICODE)\n",
        "\n",
        "    def __init__(self, n: int = 5):\n",
        "        assert n >= 1 and n % 2 == 1, \"n must be odd (1,3,5,...)\"\n",
        "        self.n = n\n",
        "        self.k = n // 2\n",
        "        self.counts = Counter()\n",
        "        self.total = 0.0\n",
        "        self._built = False\n",
        "\n",
        "    # ----------------------------\n",
        "    # Default mapping POS (UD -> target scheme)\n",
        "    # Можно заменить при передаче map_pos аргумента\n",
        "    # ----------------------------\n",
        "    @staticmethod\n",
        "    def map_pos_default(ud_pos):\n",
        "        \"\"\"\n",
        "        Простейшее сопоставление меток Universal POS (spaCy/UDPipe) к твоим меткам:\n",
        "        возвращает один из: S, NI, A, V, ADV, NUM, PR, CONJ, PART, INTJ, X\n",
        "        \"\"\"\n",
        "        if not ud_pos:\n",
        "            return \"X\"\n",
        "        ud = ud_pos.upper()\n",
        "        mapping = {\n",
        "            \"NOUN\": \"S\", \"PROPN\": \"S\",\n",
        "            \"PRON\": \"NI\",\n",
        "            \"ADJ\": \"A\",\n",
        "            \"VERB\": \"V\",\n",
        "            \"AUX\": \"V\",\n",
        "            \"ADV\": \"ADV\",\n",
        "            \"NUM\": \"NUM\",\n",
        "            \"ADP\": \"PR\",     # предлог\n",
        "            \"DET\": \"A\",      # определитель — мэпим к прилагательному/артиклю (можно корректировать)\n",
        "            \"CCONJ\": \"CONJ\",\n",
        "            \"SCONJ\": \"CONJ\",\n",
        "            \"PART\": \"PART\",\n",
        "            \"INTJ\": \"INTJ\",\n",
        "            \"SYM\": \"X\",\n",
        "            \"PUNCT\": \"X\",\n",
        "            \"X\": \"X\",\n",
        "            \"SPACE\": \"X\"\n",
        "        }\n",
        "        return mapping.get(ud, \"X\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Вспомогательные: чтение и сегментация\n",
        "    # ----------------------------\n",
        "    @classmethod\n",
        "    def _split_to_sentences(cls, text):\n",
        "        parts = [p.strip() for p in cls.SENT_SPLIT_RE.split(text) if p.strip()]\n",
        "        return parts\n",
        "\n",
        "    @classmethod\n",
        "    def _tokenize_words(cls, text: str) -> List[str]:\n",
        "        return cls.WORD_RE.findall(text)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Основной сборщик из уже тегированных предложений\n",
        "    # tagged_sents: iterable of sequences of POS tags (target scheme)\n",
        "    # ----------------------------\n",
        "    def build_from_tagged(self, tagged_sents: Iterable[Sequence[str]], inplace: bool = True):\n",
        "        counts = Counter()\n",
        "        total = 0.0\n",
        "        n = self.n\n",
        "        k = self.k\n",
        "        for sent in tagged_sents:\n",
        "            L = len(sent)\n",
        "            if L == 0:\n",
        "                continue\n",
        "            for i in range(L):\n",
        "                left = max(0, i - k)\n",
        "                right = min(L - 1, i + k)\n",
        "                window = list(sent[left:right+1])\n",
        "                if len(window) < n:\n",
        "                    window = window + [PAD] * (n - len(window))\n",
        "                counts[tuple(window)] += 1.0\n",
        "                total += 1.0\n",
        "        if inplace:\n",
        "            self.counts = counts\n",
        "            self.total = total\n",
        "            self._built = True\n",
        "        else:\n",
        "            return counts, total\n",
        "\n",
        "    # ----------------------------\n",
        "    # Build from raw text with provided tagger function (per-sentence)\n",
        "    # tagger_func(sentence_text) -> list of (word,pos) pairs OR a single list for that sentence\n",
        "    # map_pos: function extern_pos -> target_pos\n",
        "    # ----------------------------\n",
        "    def build_from_text_with_tagger(self, text: str, tagger_func: Callable[[str], Sequence[Tuple[str, str]]],\n",
        "                                    map_pos: Optional[Callable[[str], str]] = None):\n",
        "        if map_pos is None:\n",
        "            map_pos = self.map_pos_default\n",
        "        sentences = self._split_to_sentences(text)\n",
        "        tagged_sents = []\n",
        "        for sent in sentences:\n",
        "            try:\n",
        "                tagged = tagger_func(sent)\n",
        "            except Exception:\n",
        "                # пропускаем предложение, если теггер упал на нём\n",
        "                continue\n",
        "            # теггер может возвращать формат: list of (word,pos) for this sentence,\n",
        "            # или list of sentences (each list of pairs) — принимаем оба варианта\n",
        "            if not tagged:\n",
        "                continue\n",
        "            # Detect nested output\n",
        "            if isinstance(tagged[0], (list, tuple)) and len(tagged) > 0 and isinstance(tagged[0][0], (list, tuple)):\n",
        "                # удалось вернуть список предложений\n",
        "                for s in tagged:\n",
        "                    pos_seq = [map_pos(pos) for (_w, pos) in s if pos is not None]\n",
        "                    if pos_seq:\n",
        "                        tagged_sents.append(pos_seq)\n",
        "            else:\n",
        "                # single sentence\n",
        "                pos_seq = [map_pos(pos) for (_w, pos) in tagged if pos is not None]\n",
        "                if pos_seq:\n",
        "                    tagged_sents.append(pos_seq)\n",
        "        # build counts\n",
        "        self.build_from_tagged(tagged_sents, inplace=True)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Новый: Build model from list of file paths\n",
        "    # file_paths: list of filenames (text)\n",
        "    # tagger: Optional[Callable]; if None — попробуем spaCy ru model\n",
        "    # map_pos: Optional mapping function extern_pos -> target_pos\n",
        "    # encoding: file encoding\n",
        "    # chunk_sentences: сколько предложений буферизовать перед передачей в build_from_tagged (экономия памяти)\n",
        "    # ----------------------------\n",
        "    def build_from_files(self, file_paths: List[str],\n",
        "                         tagger: Optional[Callable[[str], Sequence[Tuple[str, str]]]] = None,\n",
        "                         map_pos: Optional[Callable[[str], str]] = None,\n",
        "                         encoding: str = 'utf-8',\n",
        "                         chunk_sentences: int = 1000):\n",
        "        \"\"\"\n",
        "        Читает файлы, разбивает на предложения, размечает с помощью tagger (по предложению),\n",
        "        буферизует pos-последовательности и вызывает build_from_tagged пакетами.\n",
        "        Если tagger is None — пробуем spaCy ru_core_news_sm автоматически.\n",
        "        \"\"\"\n",
        "        if map_pos is None:\n",
        "            map_pos = self.map_pos_default\n",
        "\n",
        "        # если tagger не передан — пробуем spaCy\n",
        "        if tagger is None:\n",
        "            try:\n",
        "                import spacy\n",
        "                # попытаемся загрузить модель ru_core_news_sm, если не найдена — exception\n",
        "                try:\n",
        "                    nlp = spacy.load(\"ru_core_news_sm\")\n",
        "                except Exception as e:\n",
        "                    # пробуем загрузить ru_core_news_sm; пользователь должен установить отдельно\n",
        "                    raise RuntimeError(\"spaCy model 'ru_core_news_sm' не найдена. Установи модель: \"\n",
        "                                       \"python -m spacy download ru_core_news_sm\") from e\n",
        "\n",
        "                def spacy_tagger(s: str):\n",
        "                    doc = nlp(s)\n",
        "                    # spaCy возвращает предложения — используем токены из doc.sents или весь doc\n",
        "                    # Формируем список (word, pos) для каждого предложения в doc.sents\n",
        "                    out = []\n",
        "                    for sent in doc.sents:\n",
        "                        sent_pairs = [(tok.text, tok.pos_) for tok in sent if tok.is_alpha or tok.pos_]\n",
        "                        if sent_pairs:\n",
        "                            out.append(sent_pairs)\n",
        "                    # Если spaCy вернул несколько предложений, возвращаем их; иначе flatten\n",
        "                    return out if out else []\n",
        "                tagger = spacy_tagger\n",
        "            except ImportError as e:\n",
        "                raise RuntimeError(\"spaCy не установлена. Укажи tagger вручную или установи spaCy.\") from e\n",
        "\n",
        "        # Now iterate files, collect sentences, tag with tagger in streaming fashion\n",
        "        buffer_pos_seqs = []\n",
        "        buffered = 0\n",
        "        for path in file_paths:\n",
        "            if not os.path.exists(path):\n",
        "                continue\n",
        "            with open(path, \"r\", encoding=encoding, errors='ignore') as f:\n",
        "                text = f.read()\n",
        "            sentences = self._split_to_sentences(text)\n",
        "            for s in sentences:\n",
        "                # call tagger on sentence; it may return a single sentence or list of sentences\n",
        "                try:\n",
        "                    tagged = tagger(s)\n",
        "                except Exception:\n",
        "                    # если теггер падает на предложении — пропускаем\n",
        "                    continue\n",
        "                if not tagged:\n",
        "                    continue\n",
        "                # normalize returned formats\n",
        "                if isinstance(tagged[0], (list, tuple)) and isinstance(tagged[0][0], (list, tuple)):\n",
        "                    # list of sentences\n",
        "                    for sent_pairs in tagged:\n",
        "                        pos_seq = [map_pos(pos) for (_w, pos) in sent_pairs if pos is not None]\n",
        "                        if pos_seq:\n",
        "                            buffer_pos_seqs.append(pos_seq)\n",
        "                            buffered += 1\n",
        "                else:\n",
        "                    # single sentence list of (w,pos)\n",
        "                    pos_seq = [map_pos(pos) for (_w, pos) in tagged if pos is not None]\n",
        "                    if pos_seq:\n",
        "                        buffer_pos_seqs.append(pos_seq)\n",
        "                        buffered += 1\n",
        "\n",
        "                # If buffer full, feed to build_from_tagged batch and clear\n",
        "                if buffered >= chunk_sentences:\n",
        "                    # accumulate counts partial\n",
        "                    # build_from_tagged with inplace=False returns counts tuple if we want; but easier —\n",
        "                    # just call build_from_tagged on this batch and let it set internal counts,\n",
        "                    # but we need to merge with existing counts; so use helper to merge\n",
        "                    self._merge_from_tagged_batch(buffer_pos_seqs)\n",
        "                    buffer_pos_seqs = []\n",
        "                    buffered = 0\n",
        "\n",
        "        # flush remaining\n",
        "        if buffer_pos_seqs:\n",
        "            self._merge_from_tagged_batch(buffer_pos_seqs)\n",
        "\n",
        "        self._built = True\n",
        "\n",
        "    # ----------------------------\n",
        "    # Вспомогательный: merge batch of tagged sentences в self.counts\n",
        "    # ----------------------------\n",
        "    def _merge_from_tagged_batch(self, tagged_sents_batch: Iterable[Sequence[str]]):\n",
        "        # считаем локальные counts и сливаем\n",
        "        local_counts = Counter()\n",
        "        local_total = 0.0\n",
        "        n = self.n\n",
        "        k = self.k\n",
        "        for sent in tagged_sents_batch:\n",
        "            L = len(sent)\n",
        "            if L == 0:\n",
        "                continue\n",
        "            for i in range(L):\n",
        "                left = max(0, i - k)\n",
        "                right = min(L - 1, i + k)\n",
        "                window = list(sent[left:right+1])\n",
        "                if len(window) < n:\n",
        "                    window = window + [PAD] * (n - len(window))\n",
        "                local_counts[tuple(window)] += 1.0\n",
        "                local_total += 1.0\n",
        "        # merge into global\n",
        "        for k, v in local_counts.items():\n",
        "            self.counts[k] += v\n",
        "        self.total += local_total\n",
        "\n",
        "    def get_count(self, pos_sequence):\n",
        "        if len(pos_sequence) != self.n:\n",
        "            raise ValueError(\"pos_sequence must have length n\")\n",
        "        return float(self.counts.get(tuple(pos_sequence), 0.0))\n",
        "\n",
        "    # находим вероятность для последовательности\n",
        "    def get_prob(self, pos_sequence, smooth = True):\n",
        "        if len(pos_sequence) != self.n:\n",
        "            raise ValueError(\"pos_sequence must have length n\")\n",
        "        c = self.counts.get(tuple(pos_sequence), 0.0)\n",
        "        if not smooth:\n",
        "            return float(c) / float(self.total) if self.total > 0 else 0.0\n",
        "        V = len(self.counts) or 1.0\n",
        "        return float(c + 1.0) / float(self.total + V)\n",
        "\n",
        "    def choose_best_pos_for_token(self, left_pos_fixed, right_pos_fixed, candidate_pos_list):\n",
        "        import math\n",
        "        best_pos = None\n",
        "        best_score = -math.inf\n",
        "        for cand in candidate_pos_list:\n",
        "            left_part = left_pos_fixed[-self.k:] if left_pos_fixed else []\n",
        "            right_part = right_pos_fixed[:self.k] if right_pos_fixed else []\n",
        "            window = list(left_part) + [cand] + list(right_part)\n",
        "            if len(window) < self.n:\n",
        "                window = window + [PAD] * (self.n - len(window))\n",
        "            p = self.get_prob(window, smooth=True)\n",
        "            score = math.log(p) if p > 0 else -math.inf\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_pos = cand\n",
        "        return best_pos, best_score\n",
        "\n",
        "    # save\n",
        "    def save(self, path: str):\n",
        "        with open(path, \"wb\") as f:\n",
        "            pickle.dump({'n': self.n, 'counts': self.counts, 'total': self.total}, f)\n",
        "\n",
        "    # load\n",
        "    def load(self, path: str):\n",
        "        with open(path, \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "        self.n = data['n']\n",
        "        self.k = self.n // 2\n",
        "        self.counts = data['counts']\n",
        "        self.total = data['total']\n",
        "        self._built = True\n"
      ],
      "metadata": {
        "id": "5CG4pcbppZic"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextPreprocess:\n",
        "    SENT_SPLIT_RE = re.compile(r'(?<=[.!?])\\s+')\n",
        "\n",
        "    def __init__(self, path_to_xml):\n",
        "        opcorpa_tree = ET.parse(path_to_xml)\n",
        "        opcorpa_tree_root = opcorpa_tree.getroot()\n",
        "\n",
        "        self.gramm_dict = {\n",
        "            \"NOUN\": \"S\",   \"NPRO\": \"NI\",\n",
        "            \"ADJF\": \"A\",   \"ADJS\": \"A\",   \"COMP\": \"A\",   \"ADJ\": \"A\",\n",
        "            \"VERB\": \"V\",   \"INFN\": \"V\",   \"GRND\": \"V\",   \"PRTF\": \"V\",   \"PRTS\": \"V\",\n",
        "            \"ADVB\": \"ADV\", \"ADV\": \"ADV\",\n",
        "            \"NUMR\": \"NUM\", \"PREP\": \"PR\",  \"CONJ\": \"CONJ\",\"PRCL\": \"PART\",\"INTJ\": \"INTJ\"\n",
        "        }\n",
        "        self.pos_gramm = self.gramm_dict.keys()\n",
        "\n",
        "        self.wordform_to_lemma = {}\n",
        "        self.create_detailed_dictionary_(opcorpa_tree_root)\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_word_(word):\n",
        "        return word.strip().lower().replace('ё', 'е')\n",
        "\n",
        "    # создаем словарь с леммами\n",
        "    def create_detailed_dictionary_(self, xml_root):\n",
        "        self.wordform_to_lemma = {}\n",
        "        for lemma in xml_root.findall('.//lemma'):\n",
        "            lemma_text = lemma.get('t')\n",
        "            if lemma_text is None:\n",
        "                head = lemma.find('l')\n",
        "                if head is not None:\n",
        "                    lemma_text = head.get('t')\n",
        "                    tags = []\n",
        "                    for g in head.findall('g'):\n",
        "                        v = g.get('v')\n",
        "                        if v:\n",
        "                            tags.append(v.upper())\n",
        "                    lemma_pos = \"OTHER\"\n",
        "                    for t in tags:\n",
        "                        if t in self.pos_gramm:\n",
        "                            lemma_pos = self.gramm_dict.get(t, \"OTHER\")\n",
        "                            break\n",
        "                else:\n",
        "                    lemma_text = None\n",
        "            if not lemma_text:\n",
        "                continue\n",
        "            form_nodes = []\n",
        "            form_nodes.extend(lemma.findall('l'))\n",
        "            form_nodes.extend(lemma.findall('f'))\n",
        "            seen = set()\n",
        "            for form in form_nodes:\n",
        "                wf = form.get('t')\n",
        "                if not wf:\n",
        "                    continue\n",
        "                key = self.normalize_word_(wf)\n",
        "                if (key, lemma_text) in seen:\n",
        "                    continue\n",
        "                seen.add((key, lemma_text))\n",
        "                entry = {'lemma': lemma_text, 'pos': lemma_pos}\n",
        "                if key in self.wordform_to_lemma.keys():\n",
        "                    if entry not in self.wordform_to_lemma[key]:\n",
        "                        self.wordform_to_lemma[key].append(entry)\n",
        "                else:\n",
        "                    self.wordform_to_lemma[key] = [entry]\n",
        "\n",
        "    # удаляем пунктуацию и токенизируем\n",
        "    @staticmethod\n",
        "    def _delete_punctuation_marks(text):\n",
        "        return re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    @staticmethod\n",
        "    def _text_to_words(text):\n",
        "        return [w for w in text.split() if w]\n",
        "\n",
        "    # получаем все леммы для слова\n",
        "    def _get_words_lemma(self, words: List[str]):\n",
        "        results = []\n",
        "        for word in words:\n",
        "            word_lower = word.lower()\n",
        "            normalize_word = self.normalize_word_(word)\n",
        "            if normalize_word in self.wordform_to_lemma.keys():\n",
        "                lemmas = self.wordform_to_lemma.get(normalize_word)\n",
        "                result = {\n",
        "                    \"word\": word,\n",
        "                    \"norm\": normalize_word,\n",
        "                    \"lemmas\": lemmas  # list of {'lemma':..., 'pos':...}\n",
        "                }\n",
        "            else:\n",
        "                result = {\n",
        "                    \"word\": word,\n",
        "                    \"norm\": normalize_word,\n",
        "                    \"lemmas\": []\n",
        "                }\n",
        "            results.append(result)\n",
        "        return results\n",
        "\n",
        "    # разбиение на предложения\n",
        "    def split_to_sentences(self, text: str) -> List[str]:\n",
        "        parts = [p.strip() for p in self.SENT_SPLIT_RE.split(text) if p.strip()]\n",
        "        return parts\n",
        "\n",
        "    # пайплайн для предложения + жадная дисамбигуация слева->направо\n",
        "    def pipeline_sentence_greedy(self, sentence_text: str, builder: POSNgramFreqBuilder):\n",
        "        \"\"\"\n",
        "        sentence_text: строка предложения (может содержать пунктуацию)\n",
        "        builder: POSNgramFreqBuilder (с n заданным)\n",
        "        \"\"\"\n",
        "        # Удаляем пунктуацию для лемматизации (как было в изначальном pipeline)\n",
        "        text_wo_punct = self._delete_punctuation_marks(sentence_text)\n",
        "        words = self._text_to_words(text_wo_punct)\n",
        "        # получаем кандидаты для каждого слова\n",
        "        lemmas_info = self._get_words_lemma(words)  # список dicts {'word','norm','lemmas'}\n",
        "        L = len(lemmas_info)\n",
        "        chosen_pos_seq = []   # уже выбранные POS (для левой части)\n",
        "        result = []\n",
        "\n",
        "        for i, tok in enumerate(lemmas_info):\n",
        "            orig_word = tok['word']\n",
        "            candidates = tok['lemmas']  # list of {'lemma','pos'}\n",
        "            if not candidates:\n",
        "                # нет кандидатов: возвращаем UNKNOWN (X) и лемму = нормализованное слово\n",
        "                chosen_pos = 'X'\n",
        "                chosen_lemma = tok['norm']\n",
        "                chosen_pos_seq.append(chosen_pos)\n",
        "                result.append({\n",
        "                    'word': orig_word,\n",
        "                    'norm': tok['norm'],\n",
        "                    'chosen_pos': chosen_pos,\n",
        "                    'chosen_lemma': chosen_lemma,\n",
        "                    'candidates': candidates\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # если однозначно — выбираем сразу\n",
        "            if len(candidates) == 1:\n",
        "                chosen_pos = candidates[0]['pos']\n",
        "                chosen_lemma = candidates[0]['lemma']\n",
        "                chosen_pos_seq.append(chosen_pos)\n",
        "                result.append({\n",
        "                    'word': orig_word,\n",
        "                    'norm': tok['norm'],\n",
        "                    'chosen_pos': chosen_pos,\n",
        "                    'chosen_lemma': chosen_lemma,\n",
        "                    'candidates': candidates\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # неоднозначность — делаем жадный выбор опираясь на LEFT контекст\n",
        "            # формируем список кандидатных POS\n",
        "            candidate_pos_list = list({e['pos'] for e in candidates if e.get('pos')})\n",
        "            # left_pos_fixed = последние k выбранных POS (если меньше — передаем короче)\n",
        "            left_fixed = chosen_pos_seq[-builder.k:] if builder is not None else []\n",
        "            right_fixed = []  # greedy-left: не используем правый контекст\n",
        "            # выбираем лучший POS по builder\n",
        "            if builder is None:\n",
        "                # если билдера для разрешения омонимии нет\n",
        "                # берем первую кандидатную пару\n",
        "                chosen_pos = candidate_pos_list[0]\n",
        "            else:\n",
        "                chosen_pos, score = builder.choose_best_pos_for_token(left_fixed, right_fixed, candidate_pos_list)\n",
        "                if chosen_pos is None:\n",
        "                    # защита на случай -math.inf или проч.\n",
        "                    chosen_pos = candidate_pos_list[0]\n",
        "            # выбираем лемму, соответствующую chosen_pos (первая в списке)\n",
        "            chosen_lemma = None\n",
        "            for e in candidates:\n",
        "                if e.get('pos') == chosen_pos:\n",
        "                    chosen_lemma = e.get('lemma')\n",
        "                    break\n",
        "            if chosen_lemma is None:\n",
        "                # если не нашли по POS — берем первую лемму\n",
        "                chosen_lemma = candidates[0].get('lemma')\n",
        "\n",
        "            chosen_pos_seq.append(chosen_pos)\n",
        "            result.append({\n",
        "                'word': orig_word,\n",
        "                'norm': tok['norm'],\n",
        "                'chosen_pos': chosen_pos,\n",
        "                'chosen_lemma': chosen_lemma,\n",
        "                'candidates': candidates\n",
        "            })\n",
        "\n",
        "        return result\n",
        "\n",
        "    def pipeline_text_greedy(self, text: str, builder: POSNgramFreqBuilder):\n",
        "        sentences = self.split_to_sentences(text)\n",
        "        out = []\n",
        "        for sent in sentences:\n",
        "            sent_res = self.pipeline_sentence_greedy(sent, builder)\n",
        "            out.append({'sentence': sent, 'tokens': sent_res})\n",
        "        return out"
      ],
      "metadata": {
        "id": "mNUahOm3e-bL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.load('ru_core_news_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLvLbs4RfOqq",
        "outputId": "7c3dd7e2-d80d-4049-d890-4cea841ae663"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.ru.Russian at 0x7a5d347bd2e0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "builder = POSNgramFreqBuilder(n=5)\n",
        "builder.build_from_files([\"voyna_i_mir.txt\"])  # по умолчанию использует spaCy+map_pos_default\n",
        "builder.save(\"pos5_counts.pkl\")"
      ],
      "metadata": {
        "id": "EnIrsPFpfnfP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp = TextPreprocess(\"dict.opcorpora.xml\")"
      ],
      "metadata": {
        "id": "iR5GqKYHgv4I"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Все были рады. Он был очень рад, но не слишком долго.\""
      ],
      "metadata": {
        "id": "tEfHMVMDhgUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Стала стабильнее экономическая и политическая обстановка, предприятия вывели из тени зарплаты сотрудников. Все Гришины одноклассники уже побывали за границей, он был чуть ли не единственным, кого не вывозили никуда дальше Красной Пахры.\"\n"
      ],
      "metadata": {
        "id": "pi7oQPdFh2hX"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Я люблю русскую печь и печь пироги.\""
      ],
      "metadata": {
        "id": "Gaeog5-3ikYp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = tp.pipeline_text_greedy(text, builder)\n",
        "# res — список предложений, где для каждого токена есть chosen_pos и chosen_lemma\n",
        "for sent in res:\n",
        "    print(\"SENTENCE:\", sent['sentence'])\n",
        "    for tok in sent['tokens']:\n",
        "        print(f\"{tok['word']}({tok['chosen_lemma']}={tok['chosen_pos']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq3mcbonh-PF",
        "outputId": "25b9a0db-7030-4d96-bebe-8232b9244afe"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SENTENCE: Я люблю русскую печь и печь пироги.\n",
            "Я(я=S)\n",
            "люблю(люблю=V)\n",
            "русскую(русский=A)\n",
            "печь(печь=S)\n",
            "и(и=S)\n",
            "печь(печь=V)\n",
            "пироги(пирог=S)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UIllkGyqh-eZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}